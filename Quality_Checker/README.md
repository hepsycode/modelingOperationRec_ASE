# Quality Checker Component

To evaluate synthetic data generated through LLMs, we developed python code to collect quantitative metric related to  diversity, correctness, and hallucination, as defined as follow. 

\subsubsection*{\textbf{Correctness Metric}} 

The \textit{Correctness} metric measures whether the data instance is related to the given label. Existing approaches for measuring correctness can be divided into two categories: automatic evaluation and human evaluation. Human evaluation has been conducted by prompt engineers to self-tune the \textit{Few-shot In-Context Learning} component. Automatic evaluation has been implemented to check the correctness of event syntax using the following metric:

\begin{equation}\label{eq:eq1}
\footnotesize
\begin{aligned}
    C(\tau^{+}_{j}) = \frac{\sum_{k=1}^{s} c(e^{+}_{j,k})}{|\tau^{+}_{j}|}, \ \ \text{where} \ \
    c(e^{+}_{j,k}) = 
    \begin{cases}
          1 \ \text{if $e^{+}_{j,k}$ has correct syntax}  \\
          0 \ \text{otherwise}
    \end{cases}
\end{aligned}
\normalsize
\end{equation}

This metric can be evaluated on the full $\tau^{+}_{j}$ synthetic trace while it is possible to cluster the metrics w.r.t syntax features (i.e., MER metamodel classes).

\subsubsection*{\textbf{Diversity Metric}} \label{diversity_metrics}

\textit{Diversity} measures the difference between a chunk of text and another in the generated instances. In this work, we evaluate differences between $\tau^{+}_{j}$ synthetic traces generated by LLMs and real $\tau_{j}$ traces generated by designers using the MER component. The considered metrics are the follows: 

\begin{itemize}[leftmargin=*]
    \item \textbf{Edit-based} similarities, also known as distance-based, measure the minimum number of single-character operations (e.g., insertions, deletions, or substitutions) required to transform one string into another. 
    \begin{itemize}[leftmargin=*]
        \item \textit{Levenshtein}: Given two traces $\tau_{j}$ and $\tau^{+}_{j}$, the Levenshtein distance $dist(\tau_{j},\tau^{+}_{j})$ between them is the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one trace into the other. Starting from the Levenshtein distance, the Levenshtein similarity is defined as follows:
        \begin{equation}\label{eq:eq2}
        \resizebox{0.5\hsize}{!}{$
            \text{LEV}(\tau_{j},\tau^{+}_{j}) = 1.0 - \frac{dist(\tau_{j},\tau^{+}_{j})}{max(|\tau_{j}|,|\tau^{+}_{j}|)}
        $}
        \end{equation}
        \item \textit{Longest Common substrings (LCS)}: Given real trace $\tau_{j}$ and synthetic trace $\tau^{+}_{j}$, the maximum-length common events subsequence LCS(i,k) of $\tau_{j}$ and $\tau^{+}_{j}$, considering only characters insertion and deletion, where i and k represent the prefix length of trace string $\tau_{j}[i] \in \tau_{j}$ and $\tau^{+}_{j}[k] \in \tau^{+}_{j}$, respectively, is given by:
        \begin{equation}\label{eq:eq3}
        \resizebox{0.9\hsize}{!}{$
            LCS(i,k) = 
            \begin{cases}
                0 & \text{if} \ i = 0 \vee k = 0 \\
                LCS(i-1,k-1) + 1 & \text{if} \ i, k > 0 \wedge \tau_{j}[i] = \tau^{+}_{j}[k] \\
                0 & \text{if} \  i, k > 0 \wedge \tau_{j}[i] \neq \tau^{+}_{j}[k] % max(LCS(i,k-1),LCS(i-1,k-1))
            \end{cases} 
        $}
        \end{equation}
        \item \textit{Jaroâ€“Winkler}: The Jaro Similarity is calculated using the following formula:
        \begin{equation}\label{eq:eq4}
        \resizebox{0.75\hsize}{!}{$
            \text{JARO}(\tau_{j},\tau^{+}_{j}) = 
            \begin{cases}
                0 & \text{if $m = 0$} \\
                \frac{1}{3} \left( \frac{m}{|\tau_{j}|} + \frac{m}{|\tau^{+}_{j}|} + \frac{m-t}{m} \right) & \text{Otherwise}
            \end{cases}
        $}
        \end{equation}
        where m is the number of matching characters between $\tau_{j}$ and $\tau^{+}_{j}$ and t is half the number of transpositions.
    \end{itemize}
    \item \textbf{Token-based} similarities focus on comparing strings based on their constituent tokens or words, rather than individual characters. 
    \begin{itemize}
        \item \textit{Jaccard}: measure the size of the intersection divided by the size of the union of the strings, as follows:
        \begin{equation}\label{eq:eq5}
            \text{JACCARD}(\tau_{j},\tau^{+}_{j}) = \frac{|\tau_{j} \cap \tau^{+}_{j}|}{|\tau_{j}| + |\tau^{+}_{j}| - |\tau_{j} \cap \tau^{+}_{j}|}
        \end{equation}
        \item \textit{Sorensen-Dice}: evaluate twice the number of elements common to both traces divided by the sum of the number of elements in each trace, as follows:
        \begin{equation}\label{eq:eq6}
            \text{DICE}(\tau_{j},\tau^{+}_{j}) = \frac{2 |\tau_{j} \cap \tau^{+}_{j}|}{|\tau_{j}| + |\tau^{+}_{j}|}
        \end{equation}
        \item  \textit{Q-Gram}: count the number of the occurrences of different q-grams in the two traces. Given a trace $\tau_{j}$ and let $v \in \Psi^q$ a q-gram, the total number of occurrences of v in $\tau_{j}$, denoted by G($\tau_{j}[v]$), is obtained by sliding a window of length q over the trace tokens. Given two traces $\tau_{j}$ and $\tau^{+}_{j}$, the Q-gram similarity is described as follows:
        \begin{equation}\label{eq:eq7}
            \text{Q-GRAM}(\tau_{j},\tau^{+}_{j}) = 1 - \frac{\sum\limits_{v \in  \Psi^q}|G(\tau_{j})[v] - G(\tau^{+}_{j})[v]|}{\sum\limits_{v \in  \Psi^q} max(G(\tau_{j})[v],G(\tau^{+}_{j})[v])}
        \end{equation}
        the traces are the closer relatives the more they have q-grams in common. 
    \end{itemize}
    \item \textbf{Cosine}: similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them, as follows:
    \begin{equation}\label{eq:eq8}
        \text{COSINE}(\tau_{j},\tau^{+}_{j}) = cos(\theta) = \frac{\tau_{j} \cdot \tau^{+}_{j}}{||\tau_{j}|| \cdot ||\tau^{+}_{j}||}
    \end{equation}
    Where $\tau_{j} \cdot \tau^{+}_{j}$ is the dot product between the vector $\tau_{j}$ and $\tau^{+}_{j}$, and $||\tau_{j}||$ represents the Euclidean norm of the vector $\tau_{j}$. The resulting measure of similarity spans from -1, signifying complete opposition, to 1, indicating absolute identity. A value of 0 signifies orthogonality or decorrelation, while values in between denote varying degrees of similarity or dissimilarity. For text matching, the attribute vectors $\tau_{j}$ and $\tau^{+}_{j}$ are usually the term frequency vectors of the documents.
\end{itemize}

These metrics can be used to evaluate how well LLMs can emulate both the designer's modeling approach and patterns, as well as human-based modeling approaches.

\subsubsection*{\textbf{Hallucination Metric}} \label{hall_metric}

In the scope of the paper, we define the hallucination as the number of additional operations, namely \textit{non-realistic events}, generated compared to the human ones by specifying the following metric:

\begin{comment}
\begin{equation}\label{eq:eq1}
\begin{aligned}
& \resizebox{0.5\hsize}{!}{$
    H_{<class>}(\tau^{+}_{j}) = \frac{\sum_{k=1}^{n'} H_{<class>}(e^{+}_{j,k})}{\sum_{i=1}^{n}H_{<class>}(e_{j,i})} 
    % \forall k &= 1, \cdots, \text{max}(n',n),
$} \\
& \resizebox{0.8\hsize}{!}{$
    \text{with} \ H_{<class>}(e^{+}_{j,k}) = 
    \begin{cases}
          1 \ \text{if "class"} == \text{"Fixed MM class"}   \\
          0 \ \text{otherwise}
    \end{cases}
$}
\end{aligned}
\end{equation}
\end{comment}

\begin{equation}\label{eq:eq9}
\begin{aligned}
\resizebox{0.9\hsize}{!}{$
    H(\tau_{j},\tau^{+}_{j})_{\text{<event>}} = \frac{\text{Number of Synthetic Events} \ e^{+}_{j,k} \ \text{of type <event>}}{\text{Number of Real Events} \ e_{j,k} \ \text{of type <event>}}
$}
\end{aligned}
\end{equation}

This metric can be evaluated on the full $\tau^{+}_{j}$ synthetic trace file and $\tau_{j}$ real trace file and also for all the considered DSL metamodel classes.
If these metrics are greater than 1, then the LLM produces an incorrect synthetic trace file (i.e., hallucination results, the LLM adds more classes than those present in the real trace model).
